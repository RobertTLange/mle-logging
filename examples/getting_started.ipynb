{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f9d077-64ed-4dc3-b772-498569e96d2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `mle-logging`: A Lightweight Logger for ML Experiments\n",
    "### Author: [@RobertTLange](https://twitter.com/RobertTLange)\n",
    "\n",
    "There are few things that bring me more joy, than automating and refactoring code, which I use on a daily basis. It feels empowering (when done right) and can lead to serious time savings. One key ingredient to my workflow is the logging of neural network training trajectories and their diagnostics (predictions, checkpoints, etc.). At the beginning I often only saved lists of training/validation/test losses to `.csv` files or used simple tensorboard template code. Maybe with a time string in front of the filename. But this soon became annoying as I wanted to compare runs across hyperparameters configurations and random seeds. Which brought me to a fundamental question: **What makes a good logger for Machine Learning experiments?** After looking at some of my previous projects, I drafted a list of desired properties:\n",
    "\n",
    "1. **Generality**: The logger should support different types of experiments and provide the functionality to store/retrieve their key diagnostics. This includes time-series statistics such as losses or predictive accuracy, network checkpoints of various flavors, generated figures as well as any other objects one might want to save over the course of training.\n",
    "2. **Reproducibility**: The logger should provide all the necessary information in order to reproduce the statistics stored with it. This includes the hyperparameter configuration and random seed of the trained pipeline.\n",
    "3. **Integratability**: Given that we may want to search over hyperparameters and rerun experiments over multiple random seeds, the logger has to be able to easily combine these different sub-logs.\n",
    "4. **Usability**: The API has to pleasant and intuitive to use. Furthermore, the log aggregation should require a minimal amount of information regarding the file locations. \n",
    "\n",
    "Based on these considerations and some iterations I came up with the following design: \n",
    "\n",
    "![](https://github.com/RobertTLange/mle-logging/blob/main/docs/mle_logger_structure.png)\n",
    "\n",
    "Let's now walk through the individual logging steps, how to reload the log and how to aggregate multiple logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d6d0bc-7789-439d-8ba9-e5c2619bfee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#!pip install -q mle-logging\n",
    "from mle_logging import MLELogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a178fb-431a-4f5b-9944-294459202309",
   "metadata": {},
   "source": [
    "# Storing Basic Logging Results (Stats, Checkpoints, Plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17bcbb-a382-4249-8d6f-45b0fd2efb11",
   "metadata": {},
   "source": [
    "We start by creating an instance of our logger. The logger takes a set of minimal inputs: `time_to_track`, `what_to_track`. They are both lists of strings which provide the time and statistic variable names, we want to log over time. There are also a couple more optional basic ingredients:\n",
    "\n",
    "- `time_to_print`, `what_to_print` and `print_every_k_updates`: By providing these arguments, you tell the logger what to print out onto the console after a set of `k` updates.\n",
    "- `experiment_dir`: The base directory in which you want to store all your results in.\n",
    "- `config_fname`: The filename of hyperparameter `.json` configuration for this experiment that will be copied into the `experiment_dir`. \n",
    "- `use_tboard`: A boolean which states whether to log statistics also to a TensorBoard.\n",
    "- `model_type`: A str (`jax`, `torch`, `tensorflow`, `sklearn`, `numpy`) specifying the model format to save.\n",
    "- `overwrite_experiment_dir`: Whether to overwrite/replace a previously stored log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e8ca71-41da-4884-8e3f-717cb832b935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">╭──────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">│ ███╗   ███╗██╗     ███████╗      ██╗      ██████╗  ██████╗   16/08/2021 20:49:05 │</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">│ ████╗ ████║██║     ██╔════╝      ██║     ██╔═══██╗██╔════╝   </span><a href=\"https://tinyurl.com/srpy4nrp\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">You are awesome!</span></a><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\"> </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">🤗</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\"> │</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">│ ██╔████╔██║██║     █████╗  █████╗██║     ██║   ██║██║  ███╗     </span><a href=\"https://twitter.com/RobertTLange\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">@RobertTLange</span></a><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\"> 🐦 │</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">│ ██║╚██╔╝██║██║     ██╔══╝  ╚════╝██║     ██║   ██║██║   ██║  </span><a href=\"https://roberttlange.github.io/mle-toolbox/logging/mle_logging/\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">MLE-Logging Docs</span></a><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\"> </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">📓</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\"> │</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">│ ██║ ╚═╝ ██║███████╗███████╗      ███████╗╚██████╔╝╚██████╔╝  </span><a href=\"https://github.com/RobertTLange/mle-logging/\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">MLE-Logging Repo</span></a><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\"> </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">📝</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\"> │</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fc8da0fedf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────╮ ╭──────────────╮ ╭───────────────╮ ╭────────────────╮\n",
       "│ <span style=\"font-weight: bold\">Log Directory</span>   │ │ <span style=\"font-weight: bold\">Time Tracked</span> │ │ <span style=\"font-weight: bold\">Stats Tracked</span> │ │ <span style=\"font-weight: bold\">Models Tracked</span> │\n",
       "│ experiment_dir/ │ │ <span style=\"color: #800000; text-decoration-color: #800000\">num_updates</span>  │ │ <span style=\"color: #000080; text-decoration-color: #000080\">train_loss</span>    │ │ <span style=\"color: #008000; text-decoration-color: #008000\">torch</span>          │\n",
       "╰─────────────────╯ │ <span style=\"color: #800000; text-decoration-color: #800000\">num_epochs</span>   │ │ <span style=\"color: #000080; text-decoration-color: #000080\">test_loss</span>     │ ╰────────────────╯\n",
       "                    ╰──────────────╯ ╰───────────────╯                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fc8da0fe520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate logging to experiment_dir\n",
    "log = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                what_to_track=['train_loss', 'test_loss'],\n",
    "                time_to_print=['num_updates', 'num_epochs'],\n",
    "                what_to_print=['train_loss', 'test_loss'],\n",
    "                experiment_dir=\"experiment_dir/\",\n",
    "                config_fname=None,\n",
    "                use_tboard=True,\n",
    "                model_type='torch',\n",
    "                print_every_k_updates=1,\n",
    "                overwrite_experiment_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ae784-06c7-4580-b75b-827a7836394c",
   "metadata": {},
   "source": [
    "We can then simply log some a time-series \"tick\"/timestamp by providing the key, value pairs to `log.update()`. The log will print the last provided statistics using `rich` formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04366f12-a72e-46f1-8650-7a48592a1b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                                                                </span>       \n",
       "      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"font-weight: bold\"> ⌚ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">time</span><span style=\"font-weight: bold\">        </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"font-weight: bold\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">num_updates</span><span style=\"font-weight: bold\">  </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"font-weight: bold\">  </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">num_epochs</span><span style=\"font-weight: bold\">  </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"font-weight: bold\"> 📖 </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">train_loss</span><span style=\"font-weight: bold\">  </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"font-weight: bold\">  </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">test_loss</span><span style=\"font-weight: bold\">   </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span>       \n",
       "      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> ────────────────────────────────────────────────────────────────────────────── </span>       \n",
       "      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"color: #800000; text-decoration-color: #800000\"> 08-16|20:49:06 </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"color: #800000; text-decoration-color: #800000\">      10      </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"color: #800000; text-decoration-color: #800000\">      1       </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">     0.1234     </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">    0.1235    </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span>       \n",
       "      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                                                                </span>       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fc8d9fe1460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Can't write data (no appropriate function for conversion path)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y5/n8jxv2j53tb33c7f76r2prvc0000gn/T/ipykernel_6470/1711811791.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Update the log with collected data & save it to .hdf5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_tic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_tic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/core-code/mle-logging/mle_logging/mle_logger.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mo_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_to_track\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mo_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"time\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 write_to_hdf5(self.log_save_fname,\n\u001b[0m\u001b[1;32m    270\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/time/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mo_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock_to_track\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/core-code/mle-logging/mle_logging/mle_logger.py\u001b[0m in \u001b[0;36mwrite_to_hdf5\u001b[0;34m(log_fname, log_path, data_to_log, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     h5f.create_dataset(\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_to_store\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mle-toolbox/lib/python3.9/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mle-toolbox/lib/python3.9/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdset_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't write data (no appropriate function for conversion path)"
     ]
    }
   ],
   "source": [
    "# Save some time series statistics\n",
    "time_tic = {'num_updates': 10,\n",
    "            'num_epochs': 1}\n",
    "stats_tic = {'train_loss': 0.1234,\n",
    "             'test_loss': 0.1235}\n",
    "\n",
    "# Update the log with collected data & save it to .hdf5\n",
    "log.update(time_tic, stats_tic)\n",
    "log.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34694726-024b-4a86-9026-051f943931cf",
   "metadata": {},
   "source": [
    "Furthermore, we can save the most current checkpoint via `log.save_model()`. Here is an example for how to store a torch model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe491a-2022-4b39-8aee-21cb03f0636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model (torch, sklearn, jax, numpy)\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DummyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = DummyModel()\n",
    "log.save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f6185-15d6-44d0-b79d-4558457e7dc5",
   "metadata": {},
   "source": [
    "If you would like to save a figure that was generated during your training loop, this can be done via `log.save_plot()`. More general objects can be saved as `.pkl` via `log.save_extra`. The log will keep a counter of how many figures or objects were previously saved. If you do not provide an explicit path to the function calls, this counter will be used to archive the files chronologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2d7f2-2611-4442-a86b-9ad00767478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a matplotlib figure as .png\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.random.normal(0, 1, 20))\n",
    "log.save_plot(fig)\n",
    "# You can also explicity give a name to the file\n",
    "# log.save_plot(fig, \"some_figure_path.png\")\n",
    "\n",
    "# You can also save (somewhat) arbitrary objects .pkl\n",
    "some_dict = {\"hi\" : \"there\"}\n",
    "log.save_extra(some_dict)\n",
    "\n",
    "# You can also explicity give a name to the file\n",
    "# log.save_extra(some_dict, \"some_object_path.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab578ad-26e1-47d7-90fb-66fa4d605a84",
   "metadata": {},
   "source": [
    "And obviously you do not need to go through these steps individually, but can also save statistics, model checkpoint, a plot and extra content all in one go: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a368322-1285-46dd-bd0f-81d0dde2429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or do everything in one go\n",
    "log.update(time_tic, stats_tic, model, fig, some_dict, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40cf1a-9342-46b5-bf81-131ffda7707f",
   "metadata": {},
   "source": [
    "# Reloading for Post-Processing\n",
    "\n",
    "Reload can be done via `load_log(<experiment_dir>)`. The reloaded log is a `dotmap` dictionary, which has three subkeys: \n",
    "\n",
    "1. `meta`: The meta-information of the experiment. This includes random seed, configuration path, figure paths, experiment directory, etc..\n",
    "2. `time`: The time-series for the `time` variables.\n",
    "3. `stats`: The time-series for the `stats` variables.\n",
    "\n",
    "The individual data can be accessed via indexing with `.` - have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9744cbb-db96-40e2-ae3e-da08fb1b7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mle_logging import load_log\n",
    "\n",
    "log = load_log(\"experiment_dir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d445e-2e92-4541-931b-72fe70bb0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.meta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ecfe6-1ef7-4952-8ac7-3da430d7af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.meta.model_ckpt[0].decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267033f2-04d1-45d8-bddb-f8c98233dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38b10b-7f7f-4325-9442-6222be32b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.stats.test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5a107-e0ec-4d37-b4d7-f6f11704bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.time.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ee5cb-5867-40d6-a1ce-606577495370",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.time.num_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7919c5-5506-41af-87c5-e5d42bcf055d",
   "metadata": {},
   "source": [
    "# Log Different Random Seeds for Same Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8435a-25ae-4334-b5f8-4c666d54df5f",
   "metadata": {},
   "source": [
    "If you provide a `.json` file path and a seed_id, the log will be created in a sub-directory. Furthermore, the `.json` file will be copied for reproducibility. Multiple simultanous runs (different seeds) can now log to the same directory. Everything else remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95005465-36cb-4ba6-9bf9-0d1e171ac2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logging to experiment_dir for two random seeds\n",
    "log_seed_1 = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                       what_to_track=['train_loss', 'test_loss'],\n",
    "                       experiment_dir=\"multi_seed_dir/\",\n",
    "                       config_fname=\"config_1.json\",     # Provide path to .json config\n",
    "                       seed_id=\"seed_1\")                 # Provide some seed identifier (str)   \n",
    "\n",
    "log_seed_2 = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                       what_to_track=['train_loss', 'test_loss'],\n",
    "                       experiment_dir=\"multi_seed_dir/\",\n",
    "                       config_fname=\"config_1.json\",     # Provide path to .json config\n",
    "                       seed_id=\"seed_2\")                 # Provide some seed identifier (str)   \n",
    "\n",
    "# Save some time series statistics\n",
    "time_tic = {'num_updates': 10,\n",
    "            'num_epochs': 1}\n",
    "stats_tic = {'train_loss': 0.1234,\n",
    "             'test_loss': 0.1235}\n",
    "\n",
    "# Update the log with collected data & save them to .hdf5\n",
    "log_seed_1.update(time_tic, stats_tic, save=True)\n",
    "log_seed_2.update(time_tic, stats_tic, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4cab9-fba4-4c7d-8800-11f4aa332fb7",
   "metadata": {},
   "source": [
    "We can then use `merge_seed_logs` in order to combine both `.hdf5` log files (for the different seeds) into a single file stored in `merged_path`. The `load_log` function afterwards will load this seed-merged log and the first level of the `dotmap` dictionary will give you an overview of the different random seeds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc226c-cd4f-4ecc-b9f2-abe26993d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "from mle_logging import merge_seed_logs, load_log\n",
    "\n",
    "timestr = datetime.datetime.today().strftime(\"%Y-%m-%d\")[2:]\n",
    "experiment_dir = f\"multi_seed_dir/{timestr}_config_1/\"\n",
    "merged_path = os.path.join(experiment_dir, \"logs\", \"seed_aggregated.hdf5\")\n",
    "\n",
    "# Merge different random seeds into one .hdf5 file\n",
    "merge_seed_logs(merged_path, experiment_dir)\n",
    "\n",
    "# Load the merged log - Individual seeds can be accessed via log.seed_1, etc.\n",
    "log = load_log(experiment_dir)\n",
    "log.eval_ids, log.seed_1.stats.train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b2fb0-b8f7-4d3c-9c73-a2396dac75ab",
   "metadata": {},
   "source": [
    "You can also directly aggregate these different random seeds by setting `aggregate_seeds=True`. This will compute the mean, standard deviation as well as different percentiles over the random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42329658-6c45-433f-8ea0-1e985d8a751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged log and aggregate over random seeds (compute stats)\n",
    "log = load_log(experiment_dir, aggregate_seeds=True)\n",
    "log.eval_ids, log.stats.train_loss.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9b222-2e53-4c96-a1b9-2a1b43d553ad",
   "metadata": {},
   "source": [
    "## Log Different Configurations with Different Random Seeds\n",
    "\n",
    "Next, we can also combine different logs for different hyperparameter configurations and their random seeds. Let's first create two logs for two different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a8446-524d-4c33-a945-1703a915c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logging to experiment_dir for two .json configurations and two seeds\n",
    "log_c1_s1 = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                      what_to_track=['train_loss', 'test_loss'],\n",
    "                      experiment_dir=\"multi_config_dir/\",\n",
    "                      config_fname=\"config_1.json\",     # Provide path to .json config\n",
    "                      seed_id=\"seed_1\")                 # Provide some seed identifier (str)   \n",
    "\n",
    "log_c1_s2 = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                      what_to_track=['train_loss', 'test_loss'],\n",
    "                      experiment_dir=\"multi_config_dir/\",\n",
    "                      config_fname=\"config_1.json\",     # Provide path to .json config\n",
    "                      seed_id=\"seed_2\")                 # Provide some seed identifier (str)   \n",
    "\n",
    "log_c2_s1 = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                      what_to_track=['train_loss', 'test_loss'],\n",
    "                      experiment_dir=\"multi_config_dir/\",\n",
    "                      config_fname=\"config_2.json\",     # Provide path to .json config\n",
    "                      seed_id=\"seed_1\")                 # Provide some seed identifier (str)   \n",
    "\n",
    "log_c2_s2 = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                      what_to_track=['train_loss', 'test_loss'],\n",
    "                      experiment_dir=\"multi_config_dir/\",\n",
    "                      config_fname=\"config_2.json\",     # Provide path to .json config\n",
    "                      seed_id=\"seed_2\")                 # Provide some seed identifier (str)\n",
    "\n",
    "# Update the logs with collected data & save them to .hdf5\n",
    "log_c1_s1.update(time_tic, stats_tic, save=True)\n",
    "log_c1_s2.update(time_tic, stats_tic, save=True)\n",
    "log_c2_s1.update(time_tic, stats_tic, save=True)\n",
    "log_c2_s2.update(time_tic, stats_tic, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a0d0f-abf2-4531-9ca4-34b756b2d147",
   "metadata": {},
   "source": [
    "We can now first merge different random seeds for both configurations (again via `merge_seed_logs`) and then afterwards, combine the seed-aggregated logs for the two configurations via `merge_config_logs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635aa3d7-f36b-42e6-a9c0-9d2e5d05d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge different random seeds for each config into separate .hdf5 file\n",
    "merge_seed_logs(f\"multi_config_dir/{timestr}_config_1/logs/config_1.hdf5\", \n",
    "                f\"multi_config_dir/{timestr}_config_1/\")\n",
    "merge_seed_logs(f\"multi_config_dir/{timestr}_config_2/logs/config_2.hdf5\", \n",
    "                f\"multi_config_dir/{timestr}_config_2/\")\n",
    "\n",
    "# Aggregate the different merged configuration .hdf5 files into single meta log\n",
    "from mle_logging import merge_config_logs\n",
    "merge_config_logs(experiment_dir=\"multi_config_dir/\",\n",
    "                  all_run_ids=[\"config_1\", \"config_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872523d-e1b7-472b-b61e-5860e656d4fe",
   "metadata": {},
   "source": [
    "This meta-log can then be reloaded via `load_meta_log` and specifying its location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d9cd1-ac4f-4177-883a-5d0c843b1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afterwards load in the meta log object\n",
    "from mle_logging import load_meta_log\n",
    "meta_log = load_meta_log(\"multi_config_dir/meta_log.hdf5\")\n",
    "meta_log.eval_ids, meta_log.config_1.stats.test_loss.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8d109-aab5-4064-b49b-29845d67dfa0",
   "metadata": {},
   "source": [
    "Again `load_meta_log` has the option to `aggregate_seeds` or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1df37-5334-422e-aaef-11b13463fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_log = load_meta_log(\"multi_config_dir/meta_log.hdf5\", aggregate_seeds=False)\n",
    "meta_log.eval_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c532bed-1446-42ca-a696-0a9abd24fa95",
   "metadata": {},
   "source": [
    "# Logging Every k-th Checkpoint Update\n",
    "\n",
    "Next up you can choose to not only store the most recent checkpoint, but to also store every k-th one. Simply specify `save_every_k_ckpt` when instantiating the logger and the toolbox will take care of the archiving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c792f-f041-466f-bba3-6a5474917050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logging to experiment_dir\n",
    "log = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                what_to_track=['train_loss', 'test_loss'],\n",
    "                experiment_dir='every_k_dir/',\n",
    "                model_type='torch',\n",
    "                ckpt_time_to_track='num_updates',\n",
    "                save_every_k_ckpt=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb3afd-78ed-426a-9bd1-52f2ecc03491",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tic = {'num_updates': 10, 'num_epochs': 1}\n",
    "log.update(time_tic, stats_tic, model, save=True)\n",
    "\n",
    "time_tic = {'num_updates': 20, 'num_epochs': 1}\n",
    "log.update(time_tic, stats_tic, model, save=True)\n",
    "\n",
    "time_tic = {'num_updates': 30, 'num_epochs': 1}\n",
    "log.update(time_tic, stats_tic, model, save=True)\n",
    "\n",
    "time_tic = {'num_updates': 40, 'num_epochs': 1}\n",
    "log.update(time_tic, stats_tic, model, save=True)\n",
    "\n",
    "log.model_log.every_k_ckpt_list, log.model_log.every_k_storage_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198edbf6-a727-495e-b828-fbcdba7cfd21",
   "metadata": {},
   "source": [
    "# Logging Top-k Checkpoints Based on Metric\n",
    "\n",
    "Last but not least we can also choose to keep an archive of the top-k performing networks based on chosen metric. As additional input you have to provide the size of your archive `save_top_k_ckpt`, the metric `top_k_metric_name` and whether to minimize it `top_k_minimize_metric`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f8d0b-ecf4-4f9a-b953-fc85a79dc989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logging to experiment_dir\n",
    "log = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n",
    "                what_to_track=['train_loss', 'test_loss'],\n",
    "                experiment_dir=\"top_k_dir/\",\n",
    "                model_type='torch',\n",
    "                ckpt_time_to_track='num_updates',\n",
    "                save_top_k_ckpt=2,\n",
    "                top_k_metric_name=\"test_loss\",\n",
    "                top_k_minimize_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09195de-a5e5-40e6-94a8-488acf0fa505",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tic = {'num_updates': 10, 'num_epochs': 1}\n",
    "stats_tic = {'train_loss': 0.1234, 'test_loss': 0.1235}\n",
    "log.update(time_tic, stats_tic, model, save=True)\n",
    "\n",
    "time_tic = {'num_updates': 20, 'num_epochs': 1}\n",
    "stats_tic = {'train_loss': 0.1234, 'test_loss': 0.11}\n",
    "log.update(time_tic, stats_tic, model, save=True)\n",
    "\n",
    "time_tic = {'num_updates': 30, 'num_epochs': 1}\n",
    "stats_tic = {'train_loss': 0.1234, 'test_loss': 0.09}\n",
    "log.update(time_tic, stats_tic, model, save=True)\n",
    "\n",
    "time_tic = {'num_updates': 40, 'num_epochs': 1}\n",
    "stats_tic = {'train_loss': 0.1234, 'test_loss': 0.12}\n",
    "log.update(time_tic, stats_tic, model, save=True)\n",
    "\n",
    "log.model_log.top_k_performance, log.model_log.top_k_storage_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6fbef-7c8b-44d3-9802-0400e71dc148",
   "metadata": {},
   "source": [
    "So this is it. Let me know what you think! If you find a bug or are missing your favourite feature, feel free to contact me [@RobertTLange](https://twitter.com/RobertTLange) or create an issue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mle-toolbox)",
   "language": "python",
   "name": "mle-toolbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
